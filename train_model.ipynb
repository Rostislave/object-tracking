{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oijJ-T7QWJV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import efficientnet_b0\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7heKvTeJN0m",
        "outputId": "1615c543-b007-481b-9535-2118ca0329f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp8Iek_fQWJZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcMVo4qtQWJb"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/Datasets\"\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "num_classes = 2\n",
        "input_size = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJC-v4bWQWJc"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2,\n",
        "                           contrast=0.2,\n",
        "                           saturation=0.2,\n",
        "                           hue=0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnz93kbyQWJg"
      },
      "outputs": [],
      "source": [
        "dataset_full = datasets.ImageFolder(root=dataset_dir, transform=None)\n",
        "labels = dataset_full.targets\n",
        "indices = np.arange(len(labels))\n",
        "train_indices, val_indices = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    random_state=25,\n",
        "    stratify=labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsOgDH6vQWJi"
      },
      "outputs": [],
      "source": [
        "train_dataset = Subset(dataset_full, train_indices)\n",
        "val_dataset   = Subset(dataset_full, val_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6r786xYQWJi"
      },
      "outputs": [],
      "source": [
        "class SubsetWithTransform(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb_aNO2kQWJk"
      },
      "outputs": [],
      "source": [
        "train_dataset = SubsetWithTransform(train_dataset, transform=train_transforms)\n",
        "val_dataset   = SubsetWithTransform(val_dataset,   transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_U1217uQWJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f88796-950c-4690-8558-5ccb237b9f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 126MB/s] \n"
          ]
        }
      ],
      "source": [
        "model = efficientnet_b0(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tmtshaSQWJl"
      },
      "outputs": [],
      "source": [
        "in_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVcP3Z2lQWJl"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkMiZb55QWJl"
      },
      "outputs": [],
      "source": [
        "def train_step(model, criterion, optimizer, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "def valid_step(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37SnJKJXQWJm"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, criterion, optimizer, scheduler, device):\n",
        "    best_acc = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc='Epoch'):\n",
        "\n",
        "        train_loss, train_acc = train_step(model, criterion, optimizer, train_loader, device)\n",
        "        val_loss, val_acc = valid_step(model, criterion, val_loader, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {val_loss:.4f},   Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    torch.save(model.state_dict(), \"efficientnet_crows_and_squirrels.pth\")\n",
        "    print(\"Модель сохранена в efficientnet_crows_and_squirrels.pth\")\n",
        "\n",
        "    return model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szEwbQMGQWJo",
        "outputId": "7798fed9-ff52-4b05-d86d-ccd67bf430ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  10%|█         | 1/10 [03:24<30:44, 204.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1107, Train Acc: 0.9574\n",
            "Val   Loss: 0.0729,   Val Acc:   0.9942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  20%|██        | 2/10 [03:39<12:24, 93.01s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0471, Train Acc: 0.9868\n",
            "Val   Loss: 0.0706,   Val Acc:   0.9766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  30%|███       | 3/10 [03:54<06:40, 57.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0204, Train Acc: 0.9926\n",
            "Val   Loss: 0.0026,   Val Acc:   1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 4/10 [04:09<04:03, 40.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0198, Train Acc: 0.9897\n",
            "Val   Loss: 0.0703,   Val Acc:   0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 5/10 [04:24<02:36, 31.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0324, Train Acc: 0.9912\n",
            "Val   Loss: 0.0369,   Val Acc:   0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 6/10 [04:39<01:43, 25.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0162, Train Acc: 0.9985\n",
            "Val   Loss: 0.0390,   Val Acc:   0.9883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  70%|███████   | 7/10 [04:54<01:06, 22.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0102, Train Acc: 0.9956\n",
            "Val   Loss: 0.0360,   Val Acc:   0.9942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 8/10 [05:08<00:39, 19.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0040, Train Acc: 1.0000\n",
            "Val   Loss: 0.0306,   Val Acc:   0.9942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  90%|█████████ | 9/10 [05:23<00:18, 18.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0039, Train Acc: 1.0000\n",
            "Val   Loss: 0.0284,   Val Acc:   0.9942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [05:38<00:00, 33.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0046, Train Acc: 0.9985\n",
            "Val   Loss: 0.0290,   Val Acc:   0.9942\n",
            "Best val Acc: 1.0000\n",
            "Модель сохранена в efficientnet_crows_and_squirrels.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = train(model, num_epochs, criterion, optimizer, scheduler, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLNFKmgXRYdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}